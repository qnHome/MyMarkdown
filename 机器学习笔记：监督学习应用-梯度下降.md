---
title: 机器学习笔记：监督学习应用-梯度下降
date: 2018-08-24 13:37:50
categories: 机器学习基础
tags:
 - 机器学习
description: 记录机器学习基础知识。
---
>**前言**
第一课是机器学习的动机与应用观看后不再进行记录，多为介绍吴恩达老师和研究生的研究领域，以及一些机器学习的背景常识，博主已经读过周志华老师的《机器学习》书籍，已有对机器学习内容的基本了解，因此不再针对背景和太基本内容进行总结，直接记录干货，所以想要了解更为全面的相关领域知识可以学习下周志华老师的《机器学习》、图灵出版的《机器学习实战》以及《Deep Learning》。

***
**监督学习应用.梯度下降**
- 线性回归 Linear regression
- 梯度下降 Gradient descent
- 正规方程组 Normal equations

# 线性回归
首先我们假设预测结果函数$h(x)$如下
$$h(x)=h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2$$
$x$为当前样本,$x_i$分别代表了样本的某个属性值，为简明表示,这里$x_0=1$,这样$\theta_0$则代表了线性模型中的偏移量。
$$h(x)=\sum_{i=0}^n\theta_ix_i=\theta^Tx$$
n表示了特征的数量
"$\theta$ are called parameters"(文中带引号的为吴恩达教授的课堂笔记)也就是说$\theta$代表了模型中的参数，是得出最终模型的目标。

那么我们如何去求得这个$\theta$值呢。我们可在不断地最小化误差的过程中，取得$\theta$的值（当然这里还要考虑过拟合、欠拟合、样本噪声，提取特征等一系列问题，先做初步了解）。
$$\min_\theta\mathtt{J}(\theta)=\min_\theta\frac{1}{2}\sum_{i=0}^m\left(\mathtt{h}_\theta(x^{(i)})-\mathtt{y}^{(i)}\right)^2$$

# 求解算法
下面来讲述如何去求解这个最小化问题
## 搜索算法
“start with some $\theta$.(Say $\theta=\vec0$).Keep changing $\theta$ to reduce $J(\theta)$”
其实这一过程就是穷举法，通俗地讲就是逐个尝试，从0（或随机一个值）带入参数，得到的结果最小，那么就是参数的选取值。
## 批梯度下降法（batch gradient descent）
最为代表的一句“如果我想下山，那么我往哪个方向走最快”，即三维空间中（只有两个参数的情况下）梯度下降就像在这个不规则曲面上找最小一点，也就是误差最小点（这里要考虑梯度消失、梯度爆炸的情况，涉及权值共享等方法，这些内容读者可自行查阅，或者在之后的笔记中会有详解）
$\theta$初始化为$\vec0$或堆积产生的$\theta$。更新$\theta$的方法如下：
$$\theta_i:=\theta_i-\alpha\frac{\partial}{\partial\theta_i}\mathtt{J}(\theta)$$
“$:=$”为赋值号，即将右边的值赋值给左边的变量
对于这个式子的求导过程不再详述，非常简单的过程，请读者一定要亲自计算一遍，对后面的学习有很大帮助。
结果为(只有一个训练样本的计算结果)：
$$\theta_i:=\theta_i-\alpha(\mathtt{h}_{\theta}(x)-\mathtt{y})\cdot x$$
 $\alpha$控制了“迈出步子”的大小（通常$\alpha$的值是手动设置的，值过大则会越过最小值，值过小则影响效率）
 一般化到多个样本中公式为：
 $$\theta_i=\theta_i-\alpha\sum_{j=1}^m(\mathtt{h}_{\theta}(x^{(j)})-\mathtt{y}^{(j)})\cdot x_i^{(j)}$$
 $\sum_{j=1}^m(\mathtt{h}_{\theta}(x^{(j)})-\mathtt{y}^{(j)})\cdot x_i^{(j)}$即为$\frac{\partial}{\partial\theta_i}\mathtt{J}(\theta)$的结果
 ## 随机梯度下降算法（stochastic gradient descent）
 当$m$的值过大，每走一步就要遍历一遍训练集时，可想而知其运行效率，尤其数据集高达亿级时，处理一亿个数据求和还未能完成第一步，这时我们就要考虑随机梯度下降算法（增量梯度下降法）了。
>repeat{for j=1 to m{
$$\theta_i=\theta_i-\alpha(\mathtt{h}_{\theta}(x^{(j)})-\mathtt{y}^{(j)})\cdot x_i^{(j)}$$
<div align="center">(for all i)</div>
}  }

这样做和批梯度下降算法的区别是，更新$\theta$前不需要遍历整个训练集，只需要将当前第j个样本用来更新(用一个样本更新了所有的“方向”的梯度)，这样的结果会使得“下坡”的路径变得曲折，但是最终会徘徊在低谷（即极小值）位置。

> *m = 训练样本集
x = 输入变量/特征
y = 输出变量/标签
(x,y）训练样本*

 ## 正规方程
定义一个梯度关于函数，他是一个n+1维的向量
$$\nabla_\theta\mathtt{J}=\begin{bmatrix}
\frac{\alpha J}{\alpha\theta_0}\\
\vdots\\
\frac{\alpha J}{\alpha\theta_n}\\
\end{bmatrix}
\in \mathbb{R}^{n+1}$$

>some fact
$$trAB = trBA$$
$$trABC = trCAB = trBCA(依次将最后一位向前提)$$
$$设f(A) = trAB,\quad \nabla_AtrAB = B^T$$
$$trA = trA^T$$
$$If\ a\in\mathbb{R},tr\ a=a$$
$$\nabla_A trABA^TC=CAB+C^TAB^T$$

下面进行分析：
$X$为样本矩阵
$$X\theta=
\begin{bmatrix}
—(x^{(1)})^T—\\
—(x^{(2)})^T—\\
\vdots\\
—(x^{(m)})^T—\\
\end{bmatrix} \theta$$
由预测结果$h(x)=h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2$可得
$$X_\theta =
\begin{bmatrix}
x^{(1)T}\theta\\
\vdots\\
x^{(m)T}\theta\\
\end{bmatrix}=
\begin{bmatrix}
h_\theta(x^{(1)})\\
\vdots\\
h_\theta(x^{(m)})\\
\end{bmatrix}$$
$\vec y$为样本标记
$$
\vec y=
\begin{bmatrix}
y^{(1)}\\
\vdots\\
y^{(m)}\\
\end{bmatrix}
$$
求$X\theta -\vec y$
$$
X\theta -\vec y=
\begin{bmatrix}
h(x^{(1)})-y^{(1)}\\
\vdots\\
h(x^{(m)})-y^{(m)}\\
\end{bmatrix}$$
$$
Real： z^Tz=\sum_iz_i^2
$$
则
$$
\frac{1}{2}(X\theta-\vec y)^T(X\theta-\vec y)
=
\frac{1}{2}\sum_i^m{(h(x^{(1)})-y^{(1)})}^2=\mathtt J(\theta)
$$
令
$$
\nabla_{\theta}\mathtt J(\theta)=^{sct} \vec0
$$
求解
$$
\nabla_{\theta}\frac{1}{2}(X\theta-y)^T(X\theta-y)=
\frac{1}{2}\nabla_{\theta}tr(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty)
$$
实数的转至依然为其本身
$$
=\frac{1}{2}[\nabla_\theta tr \theta \theta^TX^TX-\nabla_\theta tr y^TX\theta-\nabla_\theta y^TX\theta]
$$
基于前面的fact
$$
\nabla_\theta tr \theta I\theta^TX^TX=X^TX\theta I+X^TX\theta I
$$
$$
\nabla_\theta tr y
^TX\theta=X^Ty
$$
因此
$$
\nabla_\theta \mathtt J(\theta)=\frac{1}{2}[X^TX\theta+X^TX\theta-X^Ty-X^Ty]
$$
$$
=X^TX\theta-X^Ty
$$
令其为 $\vec0$
$$
X^TX\theta = X^T y
$$
称其为正规方程
最终结果为：
$$
\theta=(X^TX)^{-1}X^Ty
$$
